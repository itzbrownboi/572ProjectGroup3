{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e74d1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using louvain method to analyze community detection of the network. \n",
    "# source: https://sites.google.com/site/findcommunities/home?authuser=0\n",
    "# source: https://networkx.org/documentation/stable/reference/algorithms/community.html#module-networkx.algorithms.community.louvain\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from networkx.algorithms import community\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33798fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to store nodes and edges\n",
    "actor_names = []\n",
    "edges = []\n",
    "\n",
    "# Open the nodes file\n",
    "with open('data/nodes2.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # Skip header if present\n",
    "    # Iterate over each row in the CSV file\n",
    "    for row in csvreader:\n",
    "        # Extract actor name from the row and append to the list\n",
    "        actor_names.append(row[1])  # Assuming actorName is in the second column\n",
    "\n",
    "# Open the edges file\n",
    "with open('data/edges_weighted2.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # Skip header if present\n",
    "    # Iterate over each row in the CSV file\n",
    "    for row in csvreader:\n",
    "        # Extract source and target values from the row and append as a tuple\n",
    "        edge_tuple = (row[0], row[1])  # Assuming source and target are in the first two columns\n",
    "        edges.append(edge_tuple)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fcf5c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(actor_names)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a814ae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Communities: 367\n",
      "Modularity: 0.9620386773801757\n"
     ]
    }
   ],
   "source": [
    "random.seed(2)\n",
    "\n",
    "# Detect communities using Louvain method\n",
    "communities = list(community.louvain_communities(G))\n",
    "\n",
    "# Assign unique ID to each node\n",
    "partitions = {node: idx for idx, com in enumerate(communities) for node in com}\n",
    "\n",
    "# #Print the commmunities\n",
    "# print(communities)\n",
    "\n",
    "# # Print the partitions\n",
    "# for node, community_id in partition.items():\n",
    "#     print(f\"Node {node} belongs to community {community_id}\")\n",
    "\n",
    "\n",
    "# number of communities\n",
    "num_communities = len(set(partitions.values()))\n",
    "print(\"Number of Communities:\", num_communities)\n",
    "\n",
    "# Calculate modularity\n",
    "modularity = community.modularity(G, communities)\n",
    "print(\"Modularity:\", modularity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b327e09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 303:\n",
      "Number of Nodes: 214\n",
      "Average Degree: 8.80373831775701\n",
      "Density: 0.04133210477820192\n",
      "Average Degree Centrality: 0.041332104778201914\n",
      "Average Betweenness Centrality: 0.015179775532049638\n",
      "\n",
      "Community 349:\n",
      "Number of Nodes: 137\n",
      "Average Degree: 8.671532846715328\n",
      "Density: 0.06376127093173035\n",
      "Average Degree Centrality: 0.06376127093173035\n",
      "Average Betweenness Centrality: 0.027100329182767998\n",
      "\n",
      "Community 241:\n",
      "Number of Nodes: 131\n",
      "Average Degree: 8.916030534351146\n",
      "Density: 0.06858485026423958\n",
      "Average Degree Centrality: 0.06858485026423958\n",
      "Average Betweenness Centrality: 0.019935635699882104\n",
      "\n",
      "Community 5:\n",
      "Number of Nodes: 129\n",
      "Average Degree: 8.713178294573643\n",
      "Density: 0.06807170542635659\n",
      "Average Degree Centrality: 0.06807170542635659\n",
      "Average Betweenness Centrality: 0.026529977720808152\n",
      "\n",
      "Community 11:\n",
      "Number of Nodes: 122\n",
      "Average Degree: 8.672131147540984\n",
      "Density: 0.07167050535157837\n",
      "Average Degree Centrality: 0.07167050535157839\n",
      "Average Betweenness Centrality: 0.027288533622363727\n",
      "\n",
      "Community 263:\n",
      "Number of Nodes: 122\n",
      "Average Degree: 8.967213114754099\n",
      "Density: 0.07410919929548841\n",
      "Average Degree Centrality: 0.07410919929548843\n",
      "Average Betweenness Centrality: 0.023646299056135116\n",
      "\n",
      "Community 361:\n",
      "Number of Nodes: 114\n",
      "Average Degree: 8.508771929824562\n",
      "Density: 0.0752988666356156\n",
      "Average Degree Centrality: 0.0752988666356156\n",
      "Average Betweenness Centrality: 0.02584447845276909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the size\n",
    "community_sizes = {}\n",
    "for node, comm_id in partitions.items():\n",
    "    if comm_id not in community_sizes:\n",
    "        community_sizes[comm_id] = 0\n",
    "    community_sizes[comm_id] += 1\n",
    "\n",
    "#get the 7 largest communities\n",
    "largest_communities = sorted(community_sizes.items(), key=lambda x: x[1], reverse=True)[:7]\n",
    "\n",
    "# Compute statistics for each \n",
    "for comm_id, _ in largest_communities:\n",
    "    # Nodes belonging to the current community\n",
    "    community_nodes = [node for node, comm in partitions.items() if comm == comm_id]\n",
    "    subgraph = G.subgraph(community_nodes)\n",
    "    \n",
    "    # Number of nodes \n",
    "    num_nodes = len(community_nodes)\n",
    "    \n",
    "    # Average degree \n",
    "    avg_degree = np.mean([d for n, d in subgraph.degree()])\n",
    "    \n",
    "    # Density\n",
    "    density = nx.density(subgraph)\n",
    "    \n",
    "    # Degree centrality \n",
    "    degree_centrality = nx.degree_centrality(subgraph)\n",
    "    avg_degree_centrality = np.mean(list(degree_centrality.values()))\n",
    "    \n",
    "    # Betweenness centrality \n",
    "    betweenness_centrality = nx.betweenness_centrality(subgraph)\n",
    "    avg_betweenness_centrality = np.mean(list(betweenness_centrality.values()))\n",
    "\n",
    "    print(f\"Community {comm_id}:\")\n",
    "    print(f\"Number of Nodes: {num_nodes}\")\n",
    "    print(f\"Average Degree: {avg_degree}\")\n",
    "    print(f\"Density: {density}\")\n",
    "    print(f\"Average Degree Centrality: {avg_degree_centrality}\")\n",
    "    print(f\"Average Betweenness Centrality: {avg_betweenness_centrality}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
